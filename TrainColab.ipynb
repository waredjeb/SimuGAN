{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import argparse\n",
    "import random, math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras import regularizers\n",
    "from keras.callbacks import *\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#import ROOT\n",
    "\n",
    "from models3 import *\n",
    "from help_fun import *\n",
    "from configuration import *\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description = 'train analysis')\n",
    "\n",
    "parser.add_argument('-e','--epochs', default = 1000)\n",
    "parser.add_argument('-b','--batch_size', default = 64)\n",
    "#parser.add_argument('-d','--decay', default = 1) #1 (decay G), 11(decay both), 0(no decay)\n",
    "args = parser.parse_args()\n",
    "\n",
    "n_epochs = int(args.epochs)\n",
    "batch_size = int(args.batch_size)\n",
    "#decay = int(args.decay)\n",
    "\n",
    "print(\"\\n\\n############## RUNNING TRAIN: Epochs-{}, batch_size-{}################### \\n\\n\".format(n_epochs,batch_size))\n",
    "\n",
    "#input_file = \"./Data/signal_new.npy\"\n",
    "#input_features_file = \"./Data/signal_var_new.npy\"\n",
    "\n",
    "features = np.load(features_input_file)[0,:-1]\n",
    "print(\"Features: {}\\nNumber of features: {}\".format(features,len(features)))\n",
    "\n",
    "print(\"Input_file: {}\".format(data_input_file))\n",
    "\n",
    "#data_npy = np.load(input_file)[:,0]\n",
    "data_npy = np.load(data_input_file)[:,:-1]\n",
    "data = pd.DataFrame(data_npy, columns = features)\n",
    "\n",
    "X_train = data[features].values\n",
    "print(\"X_train not standardize:\\n{}\".format(X_train))\n",
    "\n",
    "\n",
    "#scaler_name = \"./scaler/scaler_signal.pkl\"\n",
    "print(\"Loading scaler: {}\".format(scaler_input))\n",
    "\n",
    "with open(scaler_input,'rb') as scaler_in:\n",
    "    scaler = pickle.load(scaler_in)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "#Starting with the models and the train\n",
    "\n",
    "#Optimizer Generator\n",
    "#wanted_variables = np.array([\"j1_m\",\"j2_m\",\"j1_pt\",\"j2_pt\",\"j1_eta\",\"j2_eta\"])extra = \"Adam-m3-b1_05-b2_08-gaus\"\n",
    "lr = 0.1\n",
    "\n",
    "Gadam_opt = Adam(lr = 0.001, beta_1 = 0.5, beta_2 = 0.8)\n",
    "    #Optimizer Discriminator\n",
    "Dadam_opt = SGD(lr = lr)\n",
    "\n",
    "noise_input_size = 64\n",
    "Generator = generator_conv(input_size = noise_input_size, output_size = len(features))\n",
    "print(\"Generator model built!\\n ####GENERATOR####\")\n",
    "Generator.name = \"Generator\"\n",
    "Generator.compile(loss = 'mean_squared_error', optimizer = Gadam_opt)\n",
    "Generator.summary()\n",
    "\n",
    "Discriminator = discriminator_conv(output_size = len(features))\n",
    "print(\"Discriminator model built!\\n ####DISCRIMINATOR####\")\n",
    "Discriminator.name = 'Discriminator'\n",
    "Discriminator.compile(loss = 'binary_crossentropy', optimizer = Dadam_opt,\n",
    "                            metrics = ['accuracy'])\n",
    "Discriminator.summary()\n",
    "\n",
    "#To train only the generator\n",
    "Discriminator.trainable = False\n",
    "Ogen_in = Input(shape = (noise_input_size,))\n",
    "Ogen = Generator(Ogen_in)\n",
    "Ogen_out = Discriminator(Ogen)\n",
    "Ogenerator = Model(Ogen_in,Ogen_out)\n",
    "Ogenerator.name = \"Only-generator\"\n",
    "Ogenerator.compile(\n",
    "    loss = 'binary_crossentropy', optimizer = Gadam_opt\n",
    ")\n",
    "Ogenerator.summary()\n",
    "\n",
    "train_events = X_train.shape[0]\n",
    "mod_par = \"bs_%i-ept_%i-trev_%i-%s\"%(batch_size,n_epochs,train_events,extra)\n",
    "main_dir_path = \"results/\"+mod_par\n",
    "mkdir_p(main_dir_path) #Main directory for the current training.\n",
    "\n",
    "model_plot_path = main_dir_path+\"/plot_model\"\n",
    "mkdir_p(model_plot_path)\n",
    "\n",
    "plot_model(Generator , model_plot_path+\"/generator.png\")\n",
    "plot_model(Discriminator,model_plot_path+\"/discriminator.png\")\n",
    "plot_model(Ogenerator, model_plot_path+'/Gan.png')\n",
    "\n",
    "\n",
    "models_path = main_dir_path+\"/models\"\n",
    "mkdir_p(models_path)\n",
    "#Extracting 'train_events' random from X_train \n",
    "#Random indices\n",
    "train_index = random.sample(range(0,X_train.shape[0]),train_events)\n",
    "#Random events\n",
    "X_train_true = X_train[train_index,:]\n",
    "\n",
    "\n",
    "#Creating the noise sample for the generator input\n",
    "#Half true and half fake!\n",
    "X_noise = np.random.normal(0,1, size = [X_train_true.shape[0],noise_input_size])\n",
    "X_train_false = Generator.predict(X_noise) #Generator not trained yet, the prediction are almost random\n",
    "\n",
    "#Final training dataset!\n",
    "X = np.concatenate((X_train_true,X_train_false))\n",
    "#Need the labels for the discriminator!\n",
    "N = X_train_false.shape[0]\n",
    "y = np.zeros([2*N])\n",
    "#y_t = np.random.uniform(0.8,1.2, size = [N,])\n",
    "#y_f = np.random.uniform(0,0.3, size = [N,])\n",
    "#y = np.concatenate((y_t,y_f))\n",
    "#print(\"Y:{}\".format(y))\n",
    "y[:N] = 1 #TRUE!\n",
    "y[N:] = 0 #FAKE!\n",
    "\n",
    "#Train the discriminator! Only a pre-train! Few epochs!\n",
    "Discriminator.trainable = True\n",
    "#Discriminator.fit(X,y, epochs = 1, batch_size= 64)\n",
    "\n",
    "#Need several losses and accuracies!\n",
    "\n",
    "history = {\n",
    "        \"d_loss\": [], \"g_loss\": [],\n",
    "        \"d_acc\": [],  \"g_acc\": [],\n",
    "        \"d_loss_t\":[], \"d_loss_f\":[],\n",
    "        \"g_loss_t\":[], \"g_loss_f\":[],\n",
    "        \"d_acc_t\": [], \"d_acc_f\":[],\n",
    "        \"d_lr\": [], \"g_lr\":[]\n",
    "}\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "#Starting with the train!\n",
    "num = 0\n",
    "y_true = np.ones((batch_size,1))\n",
    "y_false = np.zeros((batch_size,1))\n",
    "\n",
    "#y_true = np.random.uniform(0.8,1.2, size = [batch_size,1])\n",
    "#y_false = np.random.uniform(0,0.3, size = [batch_size,1])\n",
    "\n",
    "pbar = tqdm(range(0,n_epochs+1))\n",
    "for n_i in range(n_epochs+1):\n",
    "\n",
    "    d_lr = K.get_value( Discriminator.optimizer.lr)\n",
    "    #print(\"Discriminator d_lr:\",d_lr)\n",
    "    g_lr = K.get_value(Generator.optimizer.lr)\n",
    "\n",
    "    #Some real events\n",
    "    train_index = random.sample(range(0,X_train.shape[0]),batch_size)\n",
    "    X_train_true = X_train[train_index,:]\n",
    "\n",
    "    X_noise = np.random.normal(0,1,size=[batch_size,noise_input_size])\n",
    "    X_train_false = Generator.predict(X_noise)\n",
    "\n",
    "    Discriminator.trainable = True\n",
    "\n",
    "    d_loss_t, d_loss_f = Discriminator.train_on_batch(X_train_true,y_true)\n",
    "    d_acc_t,d_acc_f = Discriminator.train_on_batch(X_train_false,y_false)\n",
    "\n",
    "#The total loss function is computed as the mean between the real and the fake\n",
    "\n",
    "    d_loss = 0.5*np.add(d_loss_t,d_loss_f)\n",
    "    d_acc = 0.5*np.add(d_acc_t,d_acc_f)\n",
    "\n",
    "    history[\"d_loss\"].append(d_loss)\n",
    "    history[\"d_acc\"].append(d_acc)\n",
    "    history[\"d_loss_t\"].append(d_loss_t)\n",
    "    history[\"d_loss_f\"].append(d_loss_f)\n",
    "    history[\"d_acc_t\"].append(d_acc_t)\n",
    "    history[\"d_acc_f\"].append(d_acc_f)\n",
    "    history[\"d_lr\"].append(d_lr)\n",
    "    history[\"g_lr\"].append(g_lr)\n",
    "\n",
    "#Train generator\n",
    "\n",
    "    Discriminator.trainable = False\n",
    "\n",
    "    g_loss = Ogenerator.train_on_batch(X_noise,y_true)\n",
    "\n",
    "    history[\"g_loss\"].append(g_loss)\n",
    "\n",
    "    if(n_i % 5000 == 0 and n_i != 0): #Save model every 5k epochs\n",
    "        num += 1\n",
    "        print(\"Discriminator d_lr:\",d_lr)\n",
    "        print(\"Generator d_lr:\",K.get_value(Generator.optimizer.lr))\n",
    "        print(\"Loss discriminator: {}\".format(d_loss))\n",
    "        print(\"Loss generator: {}\".format(g_loss))\n",
    "        print(\"Loss discriminator true {}\".format(d_loss_t))\n",
    "        print(\"Loss discriminator false {}\".format(d_loss_f))\n",
    "        model_output_generator = models_path+\"/%iep_%i.h5\"%(num,n_i)\n",
    "        #model_output_generator = \"gan_data/models/model_trained_gan_bs64_dec-0-0.000010-Adam-m2/%imodel_trained_gan_ep-%i_trev-%i_bs%i_dec-%i-%f-%s.h5\"%(num,n_i,train_events,batch_size,decay,decay_v,extra)\n",
    "        print(\"Saved Model:\",(model_output_generator))\n",
    "        Generator.save(model_output_generator)\n",
    "        \n",
    "    pbar.update()\n",
    "pbar.close()\n",
    "\n",
    "\n",
    "data = pd.DataFrame(data_npy, columns = features)\n",
    "X_train = data[features].values\n",
    "print(X_train)\n",
    "\n",
    "X_noise = np.random.normal(0,1, size = [X_train.shape[0],noise_input_size])\n",
    "\n",
    "prediction = Generator.predict(X_noise)\n",
    "\n",
    "prediction = scaler.inverse_transform(prediction)\n",
    "print(\"Prediction:\\n{}\".format(prediction))\n",
    "print(\"Prediction shape:{}\".format(prediction.shape))\n",
    "\n",
    "model_output_generator = models_path+\"/%iep_%i.h5\"%(num,n_i)\n",
    "#model_output_generator = \"gan_data/models/model_trained_gan_ep-%i_trev-%i_bs%i_dec-%i-%f-%s.h5\"%(n_epochs,train_events,batch_size,decay,decay_v,extra)\n",
    "Generator.save(model_output_generator)\n",
    "\n",
    "train_path = main_dir_path+\"/training\"\n",
    "mkdir_p(train_path)\n",
    "output_train = train_path+\"/training.pickle\"\n",
    "\n",
    "\n",
    "with open(output_train, 'wb') as handle:\n",
    "    pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "prediction_path = main_dir_path+\"/prediction\"\n",
    "mkdir_p(prediction_path)\n",
    "np.save(prediction_path+\"/prediction.npy\",prediction)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
